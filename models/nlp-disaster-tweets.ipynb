{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict, Counter\nfrom nltk.tokenize import word_tokenize\n\nimport re\nimport gensim\nimport string\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA PREPROCESSING\n\n# 1. Load Data\n# 2. Check Class Distribution\n# 3. Compare Numbers of Characters and Words\n# 4. Create Corpus Function\n# 5. Identify Stopwords\n# 6. Analyze Punctuation\n# 7. Find Common Words\n# 8. Ngram Analysis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Data\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_copy = train.copy() # used when training model\n\nprint('Train Shape:', train.shape)\nprint('Test Shape:', test.shape)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Class Distribution\nx = train.target.value_counts()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('Samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Numbers of Characters and Words\n\n# Number of Characters\nfig,(ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\ntweet_len = train[train['target'] == 1]['text'].str.len()\nax1.hist(tweet_len, color='red')\nax1.set_title('Disaster Tweets')\ntweet_len = train[train['target'] == 0]['text'].str.len()\nax2.hist(tweet_len, color='green')\nax2.set_title('Not Disaster Tweets')\nfig.suptitle('Number of Characters in Tweets')\nplt.show()\n\n# Number of Words\nfig,(ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\ntweet_len = train[train['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len, color='red')\nax1.set_title('Disaster Tweets')\ntweet_len = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len, color='green')\nax2.set_title('Not Disaster Tweets')\nfig.suptitle('Number of Words in Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Corpus Function\ndef create_corpus(target):\n    corpus = []\n    for x in train[train['target'] == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\n# Identify Stopwords\nfig,(ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nstop = set(stopwords.words('english'))\ncorpus0 = create_corpus(0)\ncorpus1 = create_corpus(1)\n\n# Class 0\ndic = defaultdict(int)\nfor word in corpus0:\n    if word in stop:\n        dic[word] += 1\ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)[:10]\nx, y = zip(*top)\nax1.bar(x, y, color='red')\n\n# Class 1\ndic = defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic[word]+=1\ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)[:10] \nx, y = zip(*top)\nax2.bar(x, y, color='green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze Punctuation\nfig,(ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\npunchars = string.punctuation\n\n# Class 0\ndic = defaultdict(int)\nfor i in corpus0:\n    if i in punchars:\n        dic[i] += 1\nx, y = zip(*dic.items())\nax1.bar(x, y, color='red')\n\n# Class 1\ndic = defaultdict(int)\nfor i in corpus1:\n    if i in punchars:\n        dic[i] += 1\nx, y = zip(*dic.items())\nax2.bar(x, y, color='green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find Common Words\ncounter = Counter(corpus0)\nmost = counter.most_common()\nx = []\ny = []\nfor word, count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)\nsns.barplot(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ngram Analysis\n\ndef get_top_bigrams(corpus, n = None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nplt.figure(figsize=(10,5))\ntop_bigrams = get_top_bigrams(train['text'])[:10]\nx, y = map(list, zip(*top_bigrams))\nsns.barplot(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\nnTrain = train.shape[0]\ndf = pd.concat([train,test], axis=0, sort=False)\n\n# Remove URLs\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text'] = df['text'].apply(lambda x: remove_URL(x))\n\n# Remove HTML\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text'] = df['text'].apply(lambda x: remove_html(x))\n\n# Remove Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\" u\"\\U0001F680-\\U0001F6FF\"\n                               u\"\\U0001F1E0-\\U0001F1FF\" u\"\\U00002702-\\U000027B0\" u\"\\U000024C2-\\U0001F251\" \"]+\", \n                               flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text'] = df['text'].apply(lambda x: remove_emoji(x))\n\n# Remove Punctuation\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndf['text'] = df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GloVe Vectorization\n\n# 1. Define New Create Corpus Function\n# 2. Set Up Embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define New Create Corpus Function\ndef create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus = create_corpus(df)\n\n# Set Up Embedding\nembedding_dict = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\nf.close()\n\nmax_len = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\ntweet_pad = pad_sequences(sequences, maxlen=max_len, truncating='post', padding='post')\n\nword_index = tokenizer_obj.word_index\nnum_words = len(word_index) + 1\nprint('Number of unique words:', len(word_index))\n\nembedding_matrix = np.zeros((num_words, 100))\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\n\n# 1. Baseline Model\n# 2. Plot Train and Validation\n# 3. Submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline Model\nmodel = Sequential()\nmodel.add(Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), \n                    input_length = max_len, trainable=False))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = tweet_pad[:nTrain]\ntest = tweet_pad[nTrain:]\n\nX_train, X_test, y_train, y_test = train_test_split(train, train_copy['target'], test_size=0.15)\nprint('Train:', X_train.shape)\nprint(\"Validation:\", X_test.shape)\n\nhistory = model.fit(X_train, y_train, batch_size=4, epochs=15, validation_data=(X_test, y_test), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Train and Validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training Loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"Validation Loss\")\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nsample_sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ny_pred = model.predict(test)\nprint(y_pred.shape)\ny_pred = np.round(y_pred).astype(int).reshape(3263)\nsub = pd.DataFrame({'id': sample_sub['id'].values.tolist(), 'target': y_pred})\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}