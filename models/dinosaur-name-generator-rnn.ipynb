{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus Island\n",
    "\n",
    "Using a list of dinosaur names, build a character level language model to generate new dinosaur names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "### 1.1 - Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n",
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
    "\n",
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   0: '\\n',\n",
      "    1: 'a',\n",
      "    2: 'b',\n",
      "    3: 'c',\n",
      "    4: 'd',\n",
      "    5: 'e',\n",
      "    6: 'f',\n",
      "    7: 'g',\n",
      "    8: 'h',\n",
      "    9: 'i',\n",
      "    10: 'j',\n",
      "    11: 'k',\n",
      "    12: 'l',\n",
      "    13: 'm',\n",
      "    14: 'n',\n",
      "    15: 'o',\n",
      "    16: 'p',\n",
      "    17: 'q',\n",
      "    18: 'r',\n",
      "    19: 's',\n",
      "    20: 't',\n",
      "    21: 'u',\n",
      "    22: 'v',\n",
      "    23: 'w',\n",
      "    24: 'x',\n",
      "    25: 'y',\n",
      "    26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # creates a dictionary that maps each char to an index (0-26)\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # creates a dictionary that maps each index to its char\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "1. Initialize parameters \n",
    "2. Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameters with the gradient descent update rule.\n",
    "3. Return the learned parameters \n",
    "\n",
    "\n",
    "At each time-step, the RNN tries to predict what the next character is given the previous characters. \n",
    "* $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ = a list of characters in the training set\n",
    "* $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ = the same list of characters but shifted one character forward\n",
    "\n",
    "At every $t$, $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building blocks of the model\n",
    "\n",
    "### 2.1 - Gradient Clipping\n",
    "\n",
    "**Exploding Gradients** occur when gradients are very large. They make the training process more difficult, because they can \"overshoot\" the optimal values during back propagation. When gradient clipping is performed before updating our parameters, we can ensure our gradients are not exploding. \n",
    "\n",
    "**`clip() Overview`**\n",
    "\n",
    "`Clips the gradients' values between minimum and maximum.`\n",
    "    \n",
    "**`Arguments`**\n",
    "```\n",
    "gradients = dictionary of gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "maxValue = everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "gradients = dictionary of clipped gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Sampling\n",
    "\n",
    "With sampling, we assume that our model is trained and generate new text (characters).\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Input the \"dummy\" vector of zeros $x^{\\langle 1 \\rangle} = \\vec{0}$ and set $a^{\\langle 0 \\rangle} = \\vec{0}$.\n",
    "\n",
    "\n",
    "**Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  \n",
    "\n",
    "\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "\n",
    "**Step 3**: Now that we have $y^{\\langle t+1 \\rangle}$, we want to select the next letter in the dinosaur name. We randomly select a next letter that is likely, but not always the same. We pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. \n",
    "\n",
    "\n",
    "**Step 4**: Update `x` from $x^{\\langle t \\rangle }$ to $x^{\\langle t + 1 \\rangle }$, the one-hot vector corresponding to the randomly selected character. We iterate this process until a \"\\n\" character is picked, indicating the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sample() Overview`**\n",
    "\n",
    "`Sample a sequence of characters according to a sequence of probability distributions output of the RNN`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "parameters = dictionary of parameters Waa, Wax, Wya, by, and b. \n",
    "char_to_ix = dictionary mapping each char to an index\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "indices = list of length n containing indices of sampled chars\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \n",
    "    # retrieve parameters and shapes\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    # Step 1: Create Zero Vector X \n",
    "    x = np.zeros((vocab_size, 1))          # initialize sequence generation\n",
    "    a_prev = np.zeros((n_a, 1))            # initialize a_prev\n",
    "    indices = []                           # indices of characters to generate\n",
    "    idx = -1                               # initialize idx (index of the one-hot vector x that is set to 1) \n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward Propagate X\n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        np.random.seed(counter+seed) \n",
    "        \n",
    "        # Step 3: Sample Char Index from Probability Distribution y\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite Input X with Char of `idx`\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Language Model \n",
    "\n",
    "Implement the opotimization process by performing one step of stochastic gradient descent with clipped gradients. \n",
    "\n",
    "**Helper Functions**\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    # Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    # Performs backprop to compute loss gradients with respect to parameters, and returns all hidden states.\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    # Updates parameters using the Gradient Descent Update Rule.\n",
    "    return parameters\n",
    "\n",
    "def clip(gradients, maxValue)\n",
    "    # Clips the gradients' values between minimum and maximum.\n",
    "    return gradients\n",
    "```\n",
    "**`optimize() Overview`**\n",
    "\n",
    "`Execute one step of the optimization to train the model.`    \n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "X = list of ints that maps to a character in the vocabulary\n",
    "Y = X but shifted one index to the left\n",
    "a_prev = previous hidden state\n",
    "parameters = python dictionary containing:\n",
    "                    Wax = input weight            (n_a, n_x)\n",
    "                    Waa = hidden state weight     (n_a, n_a)\n",
    "                    Wya = output weight           (n_y, n_a)\n",
    "                    b = bias                      (n_a, 1)\n",
    "                    by = output bias              (n_y, 1)\n",
    "learning_rate = model's learning rate\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "loss = value of cross-entropy loss\n",
    "gradients = python dictionary containing:\n",
    "                    dWax = Gradients of input-to-hidden weights         (n_a, n_x)\n",
    "                    dWaa -- Gradients of hidden-to-hidden weights       (n_a, n_a)\n",
    "                    dWya -- Gradients of hidden-to-output weights       (n_y, n_a)\n",
    "                    db -- Gradients of bias vector                      (n_a, 1)\n",
    "                    dby -- Gradients of output bias vector              (n_y, 1)\n",
    "a[len(X)-1] = last hidden state                                         (n_a, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)                   # forward propagate \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)                  # backpropagate\n",
    "    gradients = clip(gradients, 5)                                        # clip gradients\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)  # update parameters\n",
    "        \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Train the Model \n",
    "\n",
    "After shuffling the dataset, we sample a few randomly chosen names every 100 steps of stochastic gradient descent to see how the algorithm is doing. \n",
    "    \n",
    "**`model() Overview`**\n",
    "\n",
    "`Trains the model and generates dinosaur names.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "data = text corpus\n",
    "ix_to_char = dictionary that maps the index to a character\n",
    "char_to_ix = dictionary that maps a character to an index\n",
    "num_iterations = number of iterations to train the model for\n",
    "n_a = number of units of the RNN cell\n",
    "dino_names = number of dinosaur names you want to sample at each iteration. \n",
    "vocab_size = number of unique characters found in the text, size of the vocabulary\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "parameters = learned parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)   # initialize parameters\n",
    "    loss = get_initial_loss(vocab_size, dino_names)     # initialize loss\n",
    "    \n",
    "    # list of training examples (dino names)\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "     \n",
    "    a_prev = np.zeros((n_a, 1))     # initialize hidden state of LSTM\n",
    "     \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        # define one training example (X,Y)\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # perform one optimization step: Forward Prop > Back Prop > Clip > Update Parameters\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        # keep the loss smooth, accelerating the training\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 iterations, generate \"n\" characters to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            seed = 0\n",
    "            \n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                seed += 1\n",
    "\n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.255630\n",
      "\n",
      "Meustolkanolus\n",
      "Indabestacarospceryradwalosaurus\n",
      "Justolopinaveraterasauracoptelalenyden\n",
      "Maca\n",
      "Yusocles\n",
      "Daahosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.905483\n",
      "\n",
      "Phytronn\n",
      "Meicanstolanthus\n",
      "Mustrisaurus\n",
      "Pegalosaurus\n",
      "Yuskercis\n",
      "Egalosaurus\n",
      "Tromelosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.873854\n",
      "\n",
      "Nlyushanerohyisaurus\n",
      "Loga\n",
      "Lustrhigosaurus\n",
      "Nedalosaurus\n",
      "Yuslangosaurus\n",
      "Elagosaurus\n",
      "Trrangosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.710545\n",
      "\n",
      "Onyxromicoraurospareiosatrus\n",
      "Liga\n",
      "Mustoffankeugoptardoros\n",
      "Ola\n",
      "Yusodogongterosaurus\n",
      "Ehaerona\n",
      "Trododongxernochenhus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.604827\n",
      "\n",
      "Meustognathiterhucoplithaloptha\n",
      "Jigaadosaurus\n",
      "Kurrodon\n",
      "Mecaistheansaurus\n",
      "Yuromelosaurus\n",
      "Eiaeropeeton\n",
      "Troenathiteritaus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.714486\n",
      "\n",
      "Nhyxosaurus\n",
      "Kola\n",
      "Lvrosaurus\n",
      "Necalosaurus\n",
      "Yurolonlus\n",
      "Ejakosaurus\n",
      "Troindronykus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.647640\n",
      "\n",
      "Onyxosaurus\n",
      "Loceahosaurus\n",
      "Lustleonlonx\n",
      "Olabasicachudrakhurgawamosaurus\n",
      "Ytrojianiisaurus\n",
      "Eladon\n",
      "Tromacimathoshargicitan\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.598485\n",
      "\n",
      "Oryuton\n",
      "Locaaesaurus\n",
      "Lustoendosaurus\n",
      "Olaahus\n",
      "Yusaurus\n",
      "Ehadopldarshuellus\n",
      "Troia\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.211861\n",
      "\n",
      "Meutronlapsaurus\n",
      "Kracallthcaps\n",
      "Lustrathus\n",
      "Macairugeanosaurus\n",
      "Yusidoneraverataus\n",
      "Eialosaurus\n",
      "Troimaniathonsaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.447230\n",
      "\n",
      "Onyxipaledisons\n",
      "Kiabaeropa\n",
      "Lussiamang\n",
      "Pacaeptabalsaurus\n",
      "Xosalong\n",
      "Eiacoteg\n",
      "Troia\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shakespeare Poem Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 268s - loss: 2.5556   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb85ee588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: know thyself\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "know thyself this,\n",
      "that than thy gride tunch stepe wor's conse make.\n",
      " love heart thatel nor cinter helf my sight,\n",
      "it siviftatthe sud mesthire my beling his'.\n",
      "of thy mall naf enders may nimpece ang thee?\n",
      "ey for made end borte all my pose will thee,\n",
      "might life my to? wi the vo deed adeloned,\n",
      "and soum your of, out letge is my a do all,\n",
      "haff withret fur mear's upess geasss not,\n",
      "wimend, and my to coverses worme ha"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
