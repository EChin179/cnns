{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# numerical and categorical features\nnumerical = train.select_dtypes(include=['int64','float64'])\ncategorical = train.select_dtypes(include=['object'])\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\nf, ax = plt.subplots(figsize=(12, 9))\ncorrmat = train.corr()\nsns.heatmap(corrmat, vmax=.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sale price top 10 highest correlations matrix\ncols = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 9}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))\n\n# drop columns with above 15% missing\n# garage features expressed by GarageCars\n# basement features expressed by TotalBsmtSF\n# mas features are not essential\n# keep electrical, remove 1 missing row\n\n# dropping columns\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index, axis=1)\ntest = test.drop((missing_data[missing_data['Total'] > 1]).index, axis=1)   # whatever columns we drop in training need to be dropped in test set too\ntrain = train.drop(['Utilities'], axis=1)                                   # 1 \"NoSeWa\", 2 NA, rest \"Allpub\". 'NoSewa' is in train set, won't help in prediction. drop it.\ntest = test.drop(['Utilities'], axis=1)  \ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\nprint('Train Missing Features:', train.isnull().sum().max()) # double check\n\n# handling missing data in test set now\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))\n\ntest[\"Functional\"] = test[\"Functional\"].fillna(\"Typ\")                               # data description says NA = 'typical'\nfor col in ('MSZoning', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'SaleType'):\n    test[col] = test[col].fillna(test[col].mode()[0])\nfor col in ('BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'GarageCars', 'GarageArea', 'TotalBsmtSF'):\n    test[col] = test[col].fillna(0)\n\nprint('Test Missing Features:', test.isnull().sum().max()) # double check\n\n# Afternote: looking back, I should have concatenated the train and test set to preprocess the data together. Lesson learned.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outliers\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\n# data normalization/standardization (mean of 0, std of 1)\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis])       # newaxis = None, adds new column\nlows = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhighs = saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('Lows:')\nprint(lows)\nprint('\\nHighs:')\nprint(highs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saleprice vs grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))\n\n# note the two lower outliers. we will delete these. \n# the two higher outliers can stay since they go with the trend\n\n# deleting points\ntrain.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saleprice vs totalbsmtSF\nvar = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0, 800000))\n\n# deleting points with TotalBsmtSF > 3000\ntrain = train.drop(train[train[var] > 3000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sale price normalization/standardization\n\nfrom scipy.stats import norm\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\n\n# plot sale price\nsns.distplot(train['SalePrice'], fit=norm, ax=ax[0, 0])\nres = stats.probplot(train['SalePrice'], plot=ax[0, 1])\nprint(\"Skewness:\", train['SalePrice'].skew())\nprint(\"Kurtosis:\", train['SalePrice'].kurt())\n\n# apply log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])\n\n# transformed sale price plot\nsns.distplot(train['SalePrice'], fit=norm, ax=ax[1, 0])\nres = stats.probplot(train['SalePrice'], plot=ax[1, 1])\nprint(\"Transformed Skewness:\", train['SalePrice'].skew())\nprint(\"Transformed Kurtosis:\", train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GrLiveArea normalization/standardization\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\n\n# plot grlivarea\nsns.distplot(train['GrLivArea'], fit=norm, ax=ax[0, 0])\nres = stats.probplot(train['GrLivArea'], plot=ax[0, 1])\nprint(\"Skewness:\", train['GrLivArea'].skew())\nprint(\"Kurtosis:\", train['GrLivArea'].kurt())\n\n# apply log transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea'])\ntest['GrLivArea'] = np.log(test['GrLivArea'])\n\n# transformed grlivarea plot\nsns.distplot(train['GrLivArea'], fit=norm, ax=ax[1, 0])\nres = stats.probplot(train['GrLivArea'], plot=ax[1, 1])\nprint(\"Transformed Skewness:\", train['GrLivArea'].skew())\nprint(\"Transformed Kurtosis:\", train['GrLivArea'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TotalBsmtSF normalization/standardization\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\n\n# plot TotalBsmtSF\nsns.distplot(train['TotalBsmtSF'], fit=norm, ax=ax[0, 0])\nres = stats.probplot(train['TotalBsmtSF'], plot=ax[0, 1])\nprint(\"Skewness:\", train['TotalBsmtSF'].skew())\nprint(\"Kurtosis:\", train['TotalBsmtSF'].kurt())\n\n# TotalBsmtSF has many 0 values, which means we can't do log transformations\n# create a binary variable of HasBasement \n# take log of all non-zero observations\ntrain['HasBsmt'] = pd.Series(0, index=train.index)\ntest['HasBsmt'] = pd.Series(0, index=test.index)\ntrain.loc[train['TotalBsmtSF'] > 0,'HasBsmt'] = 1\ntest.loc[test['TotalBsmtSF'] > 0,'HasBsmt'] = 1\ntrain.loc[train['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\ntest.loc[test['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(test['TotalBsmtSF'])\n\n# transformed TotalBsmtSF price plot\nsns.distplot(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], fit=norm, ax=ax[1, 0])\nres = stats.probplot(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], plot=ax[1, 1])\nprint(\"Transformed Skewness:\", train['TotalBsmtSF'].skew())\nprint(\"Transformed Kurtosis:\", train['TotalBsmtSF'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for homoscedasticity - equal levels of variance across the range \n# use scatterplots\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nax1.scatter(train['GrLivArea'], train['SalePrice'])\nax1.set_title('SalePrice vs GrLivArea')\nax2.scatter(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], train[train['TotalBsmtSF'] > 0]['SalePrice'])\nax2.set_title('SalePrice vs TotalBsmtSF')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert categorical variables into dummy codes\n\ny_train = train.SalePrice.values\nX_train = train.drop(['SalePrice'], axis=1)\n\nnTrain = X_train.shape[0]\ndata = pd.concat((X_train, test)).reset_index(drop=True)\ndata = pd.get_dummies(data)\nprint(data.shape)\n\nX_train = data[:nTrain]\ntest = data[nTrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" MODELS \"\"\"\n\n# libraries\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffled cross validation\n\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)  \n    rmse = np.sqrt( - cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse.mean()\n\ndef r2_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)  \n    r2 = cross_val_score(model, X_train.values, y_train, scoring=\"r2\", cv=kf)\n    return r2.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LASSO regression\n\ndef lasso_target(alpha):\n    lasso = make_pipeline(RobustScaler(), Lasso(alpha=alpha, random_state=1))\n    score = r2_cv(lasso)\n    return score\n\nparams = {'alpha': (1e-5,1e-3)}\n\nlasso_bo = BayesianOptimization(f=lasso_target, pbounds=params, random_state=1)\nlasso_bo.maximize(n_iter=25, init_points=10)\nparams = lasso_bo.max['params']\nprint(params)\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha=params['alpha'], random_state=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# elastic net regression\n\ndef enet_target(alpha, l1_ratio):\n    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=1))\n    score = r2_cv(ENet)\n    return score\n\nparams = {'alpha': (1e-5,1e-3), 'l1_ratio': (0, 1)}\n\nenet_bo = BayesianOptimization(f=enet_target, pbounds=params, random_state=1)\nenet_bo.maximize(n_iter=25, init_points=10)\nparams = enet_bo.max['params']\nprint(params)\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=params['alpha'], l1_ratio=params['l1_ratio'], random_state=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kernel ridge regression\n\ndef KRR_target(alpha, degree, coef0):\n    KRR = KernelRidge(alpha=alpha, kernel='linear', degree=degree, coef0=coef0)\n    score = r2_cv(KRR)\n    return score\n\nparams = {'alpha': (0, 1), 'degree': (0, 5), 'coef0': (0, 5)}\n\nKRR_bo = BayesianOptimization(f=KRR_target, pbounds=params, random_state=1)\nKRR_bo.maximize(n_iter=25, init_points=10)\nparams = KRR_bo.max['params']\nprint(params)\n\nKRR = KernelRidge(alpha=params['alpha'], kernel='linear', degree=params['degree'], coef0=params['coef0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gradient boosting regression\n\ndef GBoost_target(n_estimators, learning_rate, max_depth, min_samples_leaf, min_samples_split):\n    GBoost = GradientBoostingRegressor(n_estimators=int(n_estimators), learning_rate=learning_rate, max_depth=int(max_depth), max_features='sqrt',\n                                       min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, loss='huber', random_state=5)\n    score = r2_cv(GBoost)\n    return score\n\nparams = {'learning_rate': (1e-4, 5e-2), 'min_samples_split': (1e-5, 0.5), \n          'min_samples_leaf': (1e-5, 0.5), 'max_depth': (3, 5), 'n_estimators': (50, 3500)}\n\nGBoost_bo = BayesianOptimization(f=GBoost_target, pbounds=params, random_state=1)\nGBoost_bo.maximize(n_iter=25, init_points=10)\nparams = GBoost_bo.max['params']\nprint(params)\n\nGBoost = GradientBoostingRegressor(n_estimators=int(params['n_estimators']), learning_rate=params['learning_rate'], max_depth=int(params['max_depth']), max_features='sqrt',\n                             min_samples_leaf=params['min_samples_leaf'], min_samples_split=params['min_samples_split'], loss='huber', random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost regression\n\ndef xgb_target(colsample_bytree, gamma, learning_rate, max_depth, min_child_weight, n_estimators, reg_alpha, reg_lambda, subsample):\n    model_xgb = xgb.XGBRegressor(colsample_bytree=colsample_bytree, gamma=gamma, learning_rate=learning_rate, max_depth=int(max_depth), \n                                 min_child_weight=min_child_weight, n_estimators=int(n_estimators), reg_alpha=reg_alpha, reg_lambda=reg_lambda,\n                                 subsample=subsample, silent=1, random_state=7, nthread=-1)\n    score = r2_cv(model_xgb)\n    return score\n\nparams = {'colsample_bytree': (0, 1), 'gamma': (0, 1), 'learning_rate': (1e-4, 5e-2), \n          'max_depth': (3, 10), 'min_child_weight': (1, 5), 'n_estimators': (50, 3000),\n          'reg_alpha': (0, 1), 'reg_lambda': (0, 1), 'subsample': (0, 1)}\n\nxgb_bo = BayesianOptimization(f=xgb_target, pbounds=params, random_state=1)\nxgb_bo.maximize(n_iter=25, init_points=10)\nparams = xgb_bo.max['params']\nprint(params)\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=params['colsample_bytree'], gamma=params['gamma'], learning_rate=params['learning_rate'], max_depth=int(params['max_depth']), \n                             min_child_weight=params['min_child_weight'], n_estimators=int(params['n_estimators']), reg_alpha=params['reg_alpha'], reg_lambda=params['reg_lambda'],\n                             subsample=params['subsample'], silent=1, random_state=7, nthread=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM\n\ndef lgb_target(num_leaves, learning_rate, n_estimators, max_bin, bagging_fraction, bagging_freq, feature_fraction, min_data_in_leaf, min_sum_hessian_in_leaf):\n    model_lgb = lgb.LGBMRegressor(num_leaves=int(num_leaves), learning_rate=learning_rate, n_estimators=int(n_estimators),\n                                  max_bin=int(max_bin), bagging_fraction=bagging_fraction, bagging_freq=int(bagging_freq), feature_fraction=feature_fraction,\n                                  feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=int(min_data_in_leaf), min_sum_hessian_in_leaf=min_sum_hessian_in_leaf)\n    score = r2_cv(model_lgb)\n    return score\n\nparams = {'num_leaves': (5, 40), 'learning_rate': (1e-4, 1), 'n_estimators': (50, 3000), 'max_bin': (2, 500), 'bagging_fraction': (0, 1), \n          'bagging_freq': (1, 10), 'feature_fraction': (0, 1), 'min_data_in_leaf': (1, 40), 'min_sum_hessian_in_leaf': (0, 1)}\n\nlgb_bo = BayesianOptimization(f=lgb_target, pbounds=params, random_state=1)\nlgb_bo.maximize(n_iter=25, init_points=10)\nparams = lgb_bo.max['params']\nprint(params)\n\nmodel_lgb = lgb.LGBMRegressor(num_leaves=int(params['num_leaves']), learning_rate=params['learning_rate'], n_estimators=int(params['n_estimators']),\n                              max_bin=int(params['max_bin']), bagging_fraction=params['bagging_fraction'], bagging_freq=int(params['bagging_freq']), \n                              feature_fraction=params['feature_fraction'], feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=int(params['min_data_in_leaf']), \n                              min_sum_hessian_in_leaf=params['min_sum_hessian_in_leaf'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE scores\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f}\\n\".format(score))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f}\\n\".format(score))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f}\\n\".format(score))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f}\\n\".format(score))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f}\\n\".format(score))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f}\\n\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking and averaging models\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # define clones of original models to fit data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    # perform predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n    \naveraged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f}\\n\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacking with a meta model\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # Fit data on clones of original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models, then create out-of-fold predictions needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # train cloned meta-model using out-of-fold predictions as features\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # perform predictions of all base models on test data \n    # use averaged predictions as meta-features for final prediction, performed by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), meta_model = lasso)\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensemble of stacked regressors, XGBoost, and LightGBM\n\n# define rmsle evaluation\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\nstacked_averaged_models.fit(X_train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(X_train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint('Stacked Regressors:', rmsle(y_train, stacked_train_pred))\n\nmodel_xgb.fit(X_train, y_train)\nxgb_train_pred = model_xgb.predict(X_train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint('XGBoost:', rmsle(y_train, xgb_train_pred))\n\nmodel_lgb.fit(X_train, y_train)\nlgb_train_pred = model_lgb.predict(X_train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint('LightGBM:', rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing different weights for ensemble\n\nprint('0.10, 0.15, 0.75:', rmsle(y_train, stacked_train_pred*0.10 + xgb_train_pred*0.15 + lgb_train_pred*0.75))\nprint('0.10, 0.10, 0.80:', rmsle(y_train, stacked_train_pred*0.10 + xgb_train_pred*0.10 + lgb_train_pred*0.80))\nprint('0.10, 0.20, 0.70:', rmsle(y_train, stacked_train_pred*0.10 + xgb_train_pred*0.20 + lgb_train_pred*0.70))\nprint('0.15, 0.25, 0.60:', rmsle(y_train, stacked_train_pred*0.15 + xgb_train_pred*0.25 + lgb_train_pred*0.60))\nprint('LightGBM:', rmsle(y_train, lgb_train_pred)) # lowest train error, yet test error = 0.13, worse than ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.10 + xgb_pred*0.10 + lgb_pred*0.80\n\nsub = pd.DataFrame()\nsub['Id'] = test['Id']\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}