{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "**Note**: Here, we use channels-first notation of shape $(m, n_C, n_H, n_W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Encode Face Images into a 128-dimensional Vector \n",
    "\n",
    "### 1.1 - Use a ConvNet  to Compute Encodings\n",
    "\n",
    "**`FRmodel Overview`**:\n",
    "\n",
    "`Load pre-trained weights that follow the Inception model to compute encodings.`\n",
    "\n",
    "- **`Input`**`: face images of shape` $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$\n",
    "- **`Output`**`: encodings of each face image of shape` $(m, 128)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - The Triplet Loss\n",
    "\n",
    "Training will use triplets of images $(A, P, N)$ that are picked from the training dataset:  \n",
    "\n",
    "- **A**nchor = picture of a person \n",
    "- **P**ositive = a picture of the same person\n",
    "- **N**egative = a picture of a different person\n",
    "\n",
    "The **triplet loss function**:\n",
    "- minimizes the distance between Anchor & Positive\n",
    "- maximizes the distance between Anchor & Negative \n",
    "\n",
    "Ensure that $A^{(i)}$ is closer to $P^{(i)}$ than to $N^{(i)}$) by at least a margin $\\alpha$:\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "To do this, we minimize the following **triplet cost**:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+Â \\tag{3}$$\n",
    "\n",
    "**Note:** Most implementations rescale encoding vectors to have L2 norm = one (i.e., $\\mid \\mid f(img)\\mid \\mid_2$=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)   # compute distance between A and P\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)   # compute distance between A and N\n",
    "    basic_loss = pos_dist - neg_dist + alpha                                      # subtract distances and add alpha\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))                               # take max of basic_loss & 0, sum over examples\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Face Verification\n",
    "\n",
    "Build a database containing one encoding vector for each person, generating the encoding through `img_to_encoding(image_path, model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`verify() Overview`**:\n",
    "\n",
    "`verify()` checks if the front-door camera picture (`image_path`) is actually the person \"identity\" on the ID card.\n",
    "\n",
    "`image_path = path to an image\n",
    "identity = string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "database = python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "model = Inception model instance in Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \n",
    "    encoding = img_to_encoding(image_path, model)                  # compute image encoding\n",
    "    dist = np.linalg.norm(encoding-database[identity])                      # compute distance with identity\n",
    "\n",
    "    if dist < 0.7:                                                 # open / close the door\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's younes, welcome in!\n",
      "(0.65939289, True)\n",
      "It's not kian, please go away\n",
      "(0.86224014, False)\n"
     ]
    }
   ],
   "source": [
    "print(verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel))\n",
    "print(verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Face Recognition\n",
    "\n",
    " **`who_is_it() Overview`**:\n",
    "\n",
    "`who_is_it() figures out if an input image is one of the authorized persons, and if so, who.`\n",
    "\n",
    "```\n",
    "Arguments:\n",
    "image_path = path to image\n",
    "database = dictionary of image encodings and names\n",
    "model = Inception model instance in Keras\n",
    "\n",
    "Returns:\n",
    "min_dist = minimum distance between image_path encoding and database encodings\n",
    "identity = name prediction for person on image_path\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \n",
    "    encoding = img_to_encoding(image_path, model)         # compute image encoding\n",
    "    min_dist = 100                                        # initialize min_dist to a large value\n",
    "    \n",
    "    for (name, db_enc) in database.items():                       # loop over database names and encodings\n",
    "        dist = np.linalg.norm(encoding-db_enc)            # compute distance between encoding and current db_enc\n",
    "        if dist < min_dist:                               # update min_dist and identity\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.659393\n",
      "(0.65939289, 'younes')\n",
      "it's bertrand, the distance is 0.467681\n",
      "(0.46768054, 'bertrand')\n"
     ]
    }
   ],
   "source": [
    "print(who_is_it(\"images/camera_0.jpg\", database, FRmodel))     # younes\n",
    "print(who_is_it(\"images/camera_1.jpg\", database, FRmodel))     # bertrand"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
