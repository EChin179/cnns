{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Setup\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ one at a time, and remember information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirectional RNN can take context from both the past and the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Propagation\n",
    "\n",
    "### Input $x$\n",
    "\n",
    "* $n_x$ = number of units in a single training example\n",
    "* $m$ = number of training examples per mini-batch\n",
    "* $T_x$ = number of time steps in input\n",
    "\n",
    "\n",
    "1. A single input example, $x^{(i)}$, is a one-dimensional input vector, such as a one-hot encoded vector with $n_x$ units. So $x^{(i)}$ would have the shape of ($n_x$,).  \n",
    "2. If we have mini-batches of 20 training examples, we can stack 20 columns of $x^{(i)}$ examples into a matrix. Our tensor is now $(n_x,m)$.\n",
    "3. A recurrent neural network has multiple time steps, indexed with $t$, and so $x$ becomes $(n_x,m,T_x)$.\n",
    "4. At each time step, we use a mini-batch of training examples. So, for each time step $t$, we use a 2D slice of shape $x^{\\langle t \\rangle}$ = $(n_x,m)$.\n",
    "\n",
    "\n",
    "### Hidden State $a$\n",
    "\n",
    "* $n_x$ = number of units in a single hidden activation layer\n",
    "\n",
    "The **hidden state** is the activation $a^{\\langle t \\rangle}$ that is passed to the RNN from one time step to another. Similar to $x$, the shape of the hidden state is $(n_{a}, m, T_x)$. Each 2D slice $a^{\\langle t \\rangle}$ is of shape $(n_{a}, m)$. \n",
    "\n",
    "### Prediction $\\hat{y}$\n",
    "\n",
    "* $n_{y}$: number of units in the vector representing the prediction\n",
    "* $m$: number of examples in a mini-batch\n",
    "* $T_{y}$: number of time steps in the prediction\n",
    "\n",
    "$\\hat{y}$ is also of shape $(n_{y}, m, T_{y})$, with 2D slice $\\hat{y}^{\\langle t \\rangle}$ having shape $(n_{y}, m)$.\n",
    "\n",
    "\n",
    "### RNN Implementation Process\n",
    "1. Implement the calculations needed for one time-step of the RNN.\n",
    "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - RNN Cell\n",
    "\n",
    "<img src=\"images/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "**`rnn_cell_forward() Overview`**:\n",
    "\n",
    "`Implements a single forward step of an RNN cell.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "xt = input data at t                                 (n_x, m)\n",
    "a_prev = hidden state at t-1                         (n_a, m)\n",
    "parameters = python dictionary containing:\n",
    "                Wax = input weight                   (n_a, n_x)\n",
    "                Waa = hidden state weight            (n_a, n_a)\n",
    "                Wya = hidden-state > output weight   (n_y, n_a)\n",
    "                ba =  inputs bias                    (n_a, 1)\n",
    "                by = hidden-state > output bias      (n_y, 1)\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "a_next = next hidden state                           (n_a, m)\n",
    "yt_pred = prediction at t,                           (n_y, m)\n",
    "cache = tuple of values needed for backprop          (a_next, a_prev, xt, parameters)\n",
    "```\n",
    "\n",
    "**`Formulas`**\n",
    "\n",
    "<center>$a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$</center>\n",
    "\n",
    "<center>$\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)    # compute next activation state\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)                     # compute output of current cell\n",
    "    cache = (a_next, a_prev, xt, parameters)                        # store values for backprop\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - RNN forward pass \n",
    "\n",
    "RNNs are essentially RNN cell repetitions, so we re-use the RNN cell T_x times. Weights and biases are re-used each time step. \n",
    "\n",
    "**`rnn_forward() Overview`**:\n",
    "\n",
    "`Implement the forward propagation of a recurrent neural network.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "x = input                                                              (n_x, m, T_x).\n",
    "a0 = initial hidden state                                              (n_a, m)\n",
    "parameters = dictionary of Waa, Wax, Wya, ba, by\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "a = hidden states                                                      (n_a, m, T_x)\n",
    "y_pred = predictions                                                   (n_y, m, T_x)\n",
    "caches = tuple of values for backprop                                  (list of caches, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \n",
    "    # initialize caches and retrieve dimensions\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "        \n",
    "    # initialize a, y_pred, and a_next\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    a_next = a0\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        # run single RNN cell at t\n",
    "        xt = x[:,:,t]\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters)\n",
    "        a[:,:,t] = a_next          # save new next hidden state in a\n",
    "        y_pred[:,:,t] = yt_pred    # save prediction in y\n",
    "        caches.append(cache)       # append cache to caches\n",
    "\n",
    "    caches = (caches, x)           # cache for backprop\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Long Short-Term Memory (LSTM)\n",
    "\n",
    "<img src=\"images/LSTM_figure4_v3a.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> **Figure 4**: LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of gates and states\n",
    "\n",
    "The **forget gate** $\\mathbf{\\Gamma}_{f}$ is a tensor that contains values between 0 and 1, used to \"forget\" an outdated state.\n",
    "* If a unit in the forget gate is close to 0, the LSTM will \"forget\" the stored state in the corresponding unit of the previous cell state.\n",
    "* If a unit in the forget gate is close to 1, the LSTM will mostly remember the corresponding value in the stored state.\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "The **candidate value** $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ is a tensor containing values from -1 to 1, with information from the current time step that *may* be stored in the current cell state $\\mathbf{c}^{\\langle t \\rangle}$. Which parts of the candidate value get passed on depends on the update gate.\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "The **update gate** $\\mathbf{\\Gamma}_{i}$ decides what aspects of $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to add to $c^{\\langle t \\rangle}$. It is a tensor containing values between 0 and 1.\n",
    "* When a unit in the update gate is close to 1, it allows the value of the candidate $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ to be passed onto the hidden state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "* When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "The **cell state** $\\mathbf{c}^{\\langle t \\rangle}$ is the memory that gets passed onto future time steps, and a combination of the previous cell state and the candidate value.\n",
    "\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "The **output gate** $\\mathbf{\\Gamma}_{o}$ decides what gets sent as the prediction (output) of the time step, containing values that range from 0 to 1.\n",
    "\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "\n",
    "The **hidden state** $\\mathbf{a}^{\\langle t \\rangle}$ gets passed to the LSTM cell's next time step. It is used to determine the three gates ($\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$) of the next time step, and $y^{\\langle t \\rangle}$.\n",
    "\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "Since the **prediction** $\\mathbf{y}^{\\langle t \\rangle}_{pred}$ in this case is a classification, we use softmax.\n",
    "\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "### 2.1 - LSTM cell\n",
    "\n",
    "**`lstm_cell_forward() Overview`**:\n",
    "\n",
    "`Implement a single forward step of the LSTM-cell.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "xt = input data at t            (n_x, m)\n",
    "a_prev = hidden state at t-1    (n_a, m)\n",
    "c_prev = memory state at t-1    (n_a, m)\n",
    "parameters = dictionary containing:\n",
    "                Wf = forget gate weight          (n_a, n_a + n_x)\n",
    "                bf = forget gate bias            (n_a, 1)\n",
    "                Wi = update gate weight          (n_a, n_a + n_x)\n",
    "                bi = update gate bias            (n_a, 1)\n",
    "                Wc = candidate weight            (n_a, n_a + n_x)\n",
    "                bc = candidate bias              (n_a, 1)\n",
    "                Wo = output gate weight          (n_a, n_a + n_x)\n",
    "                bo = output gate bias            (n_a, 1)\n",
    "                Wy = output weight               (n_y, n_a)\n",
    "                by = output bias                 (n_y, 1)\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "a_next = next hidden state       (n_a, m)\n",
    "c_next = next memory state       (n_a, m)\n",
    "yt_pred = prediction at t        (n_y, m)\n",
    "cache = tuple for backprop       (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "\n",
    "ft/it/ot = forget/update/output gates\n",
    "cct = candidate value\n",
    "c = cell state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "\n",
    "    Wf = parameters[\"Wf\"] # forget gate\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"] # candidate value\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction\n",
    "    by = parameters[\"by\"]\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "    ft = sigmoid(np.dot(Wf, concat) + bf)        # forget gate\n",
    "    it = sigmoid(np.dot(Wi, concat) + bi)        # update gate\n",
    "    cct = np.tanh(np.dot(Wc, concat) + bc)       # candidate value\n",
    "    c_next = it * cct + ft * c_prev              # cell state\n",
    "    ot = sigmoid(np.dot(Wo, concat) + bo)        # output gate\n",
    "    a_next = ot * np.tanh(c_next)                # hidden state\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by)   # compute LSTM cell prediction\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward pass for LSTM\n",
    "\n",
    "**`lstm_forward() Overview`**\n",
    "\n",
    "`Implement the forward propagation of the recurrent neural network using an LSTM-cell.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "x = input (n_x, m, T_x).\n",
    "a0 = initial hidden state (n_a, m)\n",
    "parameters = dictionary of parameters\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "a = hidden states (n_a, m, T_x)\n",
    "y = predictions (n_y, m, T_x)\n",
    "c = cell states (n_a, m, T_x)\n",
    "caches = (list of all the caches, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "\n",
    "    caches = []\n",
    "    Wy = parameters['Wy']\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    # initialize a, c, y and a_next, c_next\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    a_next = a0\n",
    "    c_next = np.zeros(a_next.shape)\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        xt = x[:,:,t]\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        c[:,:,t]  = c_next\n",
    "        y[:,:,t] = yt\n",
    "        caches.append(cache)\n",
    "            \n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
