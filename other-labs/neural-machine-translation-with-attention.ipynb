{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\") using an attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Translating Human Readable Dates into Machine Readable Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 20814.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('28 april 1992', '1992-04-28'),\n",
       " ('saturday march 13 2010', '2010-03-13'),\n",
       " ('monday november 22 1999', '1999-11-22'),\n",
       " ('1/28/18', '2018-01-28'),\n",
       " ('sunday august 3 2003', '2003-08-03'),\n",
       " ('friday april 28 2006', '2006-04-28'),\n",
       " ('09 feb 2006', '2006-02-09'),\n",
       " ('20 03 98', '1998-03-20'),\n",
       " ('sunday august 4 1991', '1991-08-04'),\n",
       " ('tuesday september 24 2013', '2013-09-24')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Preprocessing the data and mapping raw text data into index values. \n",
    "- `Tx=30` (the maximum length of the human readable date)\n",
    "- `Ty=10` (\"YYYY-MM-DD\" is 10 characters long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X`: a processed version of the human readable dates in the training set.\n",
    "    - Each character is replaced by an index mapped to the character in `human_vocab`. \n",
    "    - Each date is padded to ensure a length of $T_x$. \n",
    "    - `X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
    "- `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index mapped to it in `machine_vocab`. \n",
    "    - `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`\n",
    "    - Each index in `X` is converted to the one-hot representation.\n",
    "    - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation. \n",
    "    - `Yoh.shape = (m, Tx, len(machine_vocab))`. \n",
    "    - `len(machine_vocab) = 11` since there are 10 numeric digits (0 to 9) and the `-` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 28 april 1992\n",
      "Target date: 1992-04-28\n",
      "\n",
      "Source after preprocessing (indices): [ 5 11  0 13 27 28 21 23  0  4 12 12  5 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  3  0  1  5  0  3  9]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Machine Translation with Attention\n",
    "\n",
    "### Attention mechanism\n",
    "\n",
    "The **attention** mechanism tells an NMT model where it should pay attention to at any step. \n",
    " \n",
    "* The diagram on the left shows the attention model. \n",
    "* The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "* The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n",
    "\n",
    "#### Pre-attention and Post-attention LSTMs\n",
    "- There are two separate LSTMs: pre-attention and post-attention LSTMs.\n",
    "- **Pre-attention** Bi-LSTM is a Bi-directional LSTM that comes *before* the attention mechanism, and goes through $T_x$ time steps.\n",
    "- **Post-attention** LSTM comes *after* the attention mechanism, and goes through $T_y$ time steps, passing the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. Note that the post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input, and not the predictions from the previous time step. This is because there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date.\n",
    "\n",
    "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. \n",
    "\n",
    "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "- Recall in the lesson videos \"Attention Model\", at time 6:45 to 8:16, the definition of \"e\" as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "    - \"e\" is called the \"energies\" variable.\n",
    "    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n",
    "    - $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n",
    "    - $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "    \n",
    "**`one_step_attention() Overview`**    \n",
    "\n",
    "`Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "\"alphas\" and the hidden states \"a\" of the Bi-LSTM.`\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "context -- context vector, input of the next (post-attention) LSTM cell\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \n",
    "    s_prev = repeator(s_prev)  # repeat s_prev to be (m, Tx, n_s) so it can concatenate\n",
    "    concat = concatenator([a, s_prev])   # concatenate a and s_prev on the last axis\n",
    "    \n",
    "    e = densor1(concat)           # FC NN computes \"intermediate energies\" variable e\n",
    "    energies = densor2(e)         # FC NN computes \"energies\" variable energies\n",
    "\n",
    "    alphas = activator(energies)  # compute attention weights \"alphas\"\n",
    "    context = dotor([alphas, a])  # compute context vector to be given to post-attention LSTM-cell\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`model() Overview`**\n",
    "\n",
    "**`Arguments`**\n",
    "```\n",
    "Tx -- length of the input sequence\n",
    "Ty -- length of the output sequence\n",
    "n_a -- hidden state size of the Bi-LSTM\n",
    "n_s -- hidden state size of the post-attention LSTM\n",
    "human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "```\n",
    "**`Returns`**\n",
    "```\n",
    "model -- Keras model instance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \n",
    "    # define inputs, s0 (initial hidden state), and c0 (initial cell state)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)   # pre-attention Bi-LSTM\n",
    "    \n",
    "    for t in range(Ty):\n",
    "    \n",
    "        context = one_step_attention(a, s)   # perform one step of attention \n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])  # post-attention LSTM cell\n",
    "        out = output_layer(s)                # dense layer \n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 64)        17920       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 128)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30, 10)        1290        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 30, 1)         11          dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_2[0][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "                                                                   dense_2[2][0]                    \n",
      "                                                                   dense_2[3][0]                    \n",
      "                                                                   dense_2[4][0]                    \n",
      "                                                                   dense_2[5][0]                    \n",
      "                                                                   dense_2[6][0]                    \n",
      "                                                                   dense_2[7][0]                    \n",
      "                                                                   dense_2[8][0]                    \n",
      "                                                                   dense_2[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 33024       dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 11)            715         lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define inputs and outputs\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "\n",
    "# fit model\n",
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-05-05 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
